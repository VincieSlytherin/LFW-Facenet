{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#net and the loss function\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform(m.weight.data)\n",
    "        nn.init.constant(m.bias, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "class LossFn:\n",
    "    def __init__(self, cls_factor=1, box_factor=1, landmark_factor=1):\n",
    "        # loss function\n",
    "        self.cls_factor = cls_factor\n",
    "        self.box_factor = box_factor\n",
    "        self.land_factor = landmark_factor\n",
    "        self.loss_cls = nn.BCELoss() # binary cross entropy\n",
    "        self.loss_box = nn.MSELoss() # mean square error\n",
    "        self.loss_landmark = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def cls_loss(self,gt_label,pred_label):\n",
    "        pred_label = torch.squeeze(pred_label)\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "        # get the mask element which >= 0, only 0 and 1 can effect the detection loss\n",
    "        mask = torch.ge(gt_label,0)\n",
    "        valid_gt_label = torch.masked_select(gt_label,mask)\n",
    "        valid_pred_label = torch.masked_select(pred_label,mask)\n",
    "        return self.loss_cls(valid_pred_label,valid_gt_label)*self.cls_factor\n",
    "\n",
    "\n",
    "    def box_loss(self,gt_label,gt_offset,pred_offset):\n",
    "        pred_offset = torch.squeeze(pred_offset)\n",
    "        gt_offset = torch.squeeze(gt_offset)\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "\n",
    "        #get the mask element which != 0\n",
    "        unmask = torch.eq(gt_label,0)\n",
    "        mask = torch.eq(unmask,0)\n",
    "        #convert mask to dim index\n",
    "        chose_index = torch.nonzero(mask.data)\n",
    "        chose_index = torch.squeeze(chose_index)\n",
    "        #only valid element can effect the loss\n",
    "        valid_gt_offset = gt_offset[chose_index,:]\n",
    "        valid_pred_offset = pred_offset[chose_index,:]\n",
    "        return self.loss_box(valid_pred_offset,valid_gt_offset)*self.box_factor\n",
    "\n",
    "\n",
    "    def landmark_loss(self,gt_label,gt_landmark,pred_landmark):\n",
    "        pred_landmark = torch.squeeze(pred_landmark)\n",
    "        gt_landmark = torch.squeeze(gt_landmark)\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "        mask = torch.eq(gt_label,-2)\n",
    "\n",
    "        chose_index = torch.nonzero(mask.data)\n",
    "        chose_index = torch.squeeze(chose_index)\n",
    "\n",
    "        valid_gt_landmark = gt_landmark[chose_index, :]\n",
    "        valid_pred_landmark = pred_landmark[chose_index, :]\n",
    "        return self.loss_landmark(valid_pred_landmark,valid_gt_landmark)*self.land_factor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PNet(nn.Module):\n",
    "    ''' PNet '''\n",
    "\n",
    "    def __init__(self, is_train=False, use_cuda=True):\n",
    "        super(PNet, self).__init__()\n",
    "        self.is_train = is_train\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # backend\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # PReLU1\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool1\n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # PReLU2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),  # conv3\n",
    "            nn.PReLU()  # PReLU3\n",
    "        )\n",
    "        # detection\n",
    "        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1)\n",
    "        # bounding box regresion\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)\n",
    "        # landmark localization\n",
    "        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1)\n",
    "\n",
    "        # weight initiation with xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        label = F.sigmoid(self.conv4_1(x))\n",
    "        offset = self.conv4_2(x)\n",
    "        # landmark = self.conv4_3(x)\n",
    "\n",
    "        if self.is_train is True:\n",
    "            # label_loss = LossUtil.label_loss(self.gt_label,torch.squeeze(label))\n",
    "            # bbox_loss = LossUtil.bbox_loss(self.gt_bbox,torch.squeeze(offset))\n",
    "            return label,offset\n",
    "        #landmark = self.conv4_3(x)\n",
    "        return label, offset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RNet(nn.Module):\n",
    "    ''' RNet '''\n",
    "\n",
    "    def __init__(self,is_train=False, use_cuda=True):\n",
    "        super(RNet, self).__init__()\n",
    "        self.is_train = is_train\n",
    "        self.use_cuda = use_cuda\n",
    "        # backend\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 28, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # prelu1\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n",
    "            nn.Conv2d(28, 48, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # prelu2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n",
    "            nn.Conv2d(48, 64, kernel_size=2, stride=1),  # conv3\n",
    "            nn.PReLU()  # prelu3\n",
    "\n",
    "        )\n",
    "        self.conv4 = nn.Linear(64*2*2, 128)  # conv4\n",
    "        self.prelu4 = nn.PReLU()  # prelu4\n",
    "        # detection\n",
    "        self.conv5_1 = nn.Linear(128, 1)\n",
    "        # bounding box regression\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "        # lanbmark localization\n",
    "        self.conv5_3 = nn.Linear(128, 10)\n",
    "        # weight initiation weih xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # backend\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        # detection\n",
    "        det = torch.sigmoid(self.conv5_1(x))\n",
    "        box = self.conv5_2(x)\n",
    "        # landmark = self.conv5_3(x)\n",
    "\n",
    "        if self.is_train is True:\n",
    "            return det, box\n",
    "        #landmard = self.conv5_3(x)\n",
    "        return det, box\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ONet(nn.Module):\n",
    "    ''' RNet '''\n",
    "\n",
    "    def __init__(self,is_train=False, use_cuda=True):\n",
    "        super(ONet, self).__init__()\n",
    "        self.is_train = is_train\n",
    "        self.use_cuda = use_cuda\n",
    "        # backend\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # prelu1\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # prelu2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # conv3\n",
    "            nn.PReLU(), # prelu3\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # pool3\n",
    "            nn.Conv2d(64,128,kernel_size=2,stride=1), # conv4\n",
    "            nn.PReLU() # prelu4\n",
    "        )\n",
    "        self.conv5 = nn.Linear(128*2*2, 256)  # conv5\n",
    "        self.prelu5 = nn.PReLU()  # prelu5\n",
    "        # detection\n",
    "        self.conv6_1 = nn.Linear(256, 1)\n",
    "        # bounding box regression\n",
    "        self.conv6_2 = nn.Linear(256, 4)\n",
    "        # lanbmark localization\n",
    "        self.conv6_3 = nn.Linear(256, 10)\n",
    "        # weight initiation weih xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # backend\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv5(x)\n",
    "        x = self.prelu5(x)\n",
    "        # detection\n",
    "        det = torch.sigmoid(self.conv6_1(x))\n",
    "        box = self.conv6_2(x)\n",
    "        landmark = self.conv6_3(x)\n",
    "        if self.is_train is True:\n",
    "            return det, box, landmark\n",
    "        #landmard = self.conv5_3(x)\n",
    "        return det, box, landmark\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# ResNet Module\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = nn.Conv2d(3, 16,kernel_size=3)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, 3)\n",
    "        self.layer2 = self.make_layer(block, 32, 3, 2)\n",
    "        self.layer3 = self.make_layer(block, 64, 3, 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=3, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "# create the whole mtcnn\n",
    "def create_mtcnn_net(p_model_path=None, r_model_path=None, o_model_path=None, use_cuda=True):\n",
    "\n",
    "    pnet, rnet, onet = None, None, None\n",
    "\n",
    "    if p_model_path is not None:\n",
    "        pnet = PNet(use_cuda=use_cuda)\n",
    "        if(use_cuda):\n",
    "            print('p_model_path:{0}'.format(p_model_path))\n",
    "            pnet.load_state_dict(torch.load(p_model_path))\n",
    "            pnet.cuda()\n",
    "        else:\n",
    "            \n",
    "            pnet.load_state_dict(torch.load(p_model_path, map_location='cpu'))\n",
    "        pnet.eval()\n",
    "\n",
    "    if r_model_path is not None:\n",
    "        rnet = RNet(use_cuda=use_cuda)\n",
    "        if (use_cuda):\n",
    "            print('r_model_path:{0}'.format(r_model_path))\n",
    "            rnet.load_state_dict(torch.load(r_model_path))\n",
    "            rnet.cuda()\n",
    "        else:\n",
    "            rnet.load_state_dict(torch.load(r_model_path, map_location=lambda storage, loc: storage))\n",
    "        rnet.eval()\n",
    "\n",
    "    if o_model_path is not None:\n",
    "        onet = ONet(use_cuda=use_cuda)\n",
    "        if (use_cuda):\n",
    "            print('o_model_path:{0}'.format(o_model_path))\n",
    "            onet.load_state_dict(torch.load(o_model_path))\n",
    "            onet.cuda()\n",
    "        else:\n",
    "            onet.load_state_dict(torch.load(o_model_path, map_location=lambda storage, loc: storage))\n",
    "        onet.eval()\n",
    "\n",
    "    return pnet,rnet,onet\n",
    "\n",
    "\n",
    "class MtcnnDetector(object):\n",
    "    \"\"\"\n",
    "        P,R,O net face detection and landmarks align\n",
    "    \"\"\"\n",
    "    def  __init__(self,\n",
    "                 pnet = None,\n",
    "                 rnet = None,\n",
    "                 onet = None,\n",
    "                 min_face_size=12,\n",
    "                 stride=2,\n",
    "                 threshold=[0.6, 0.7, 0.7],\n",
    "                 scale_factor=0.709,\n",
    "                 ):\n",
    "\n",
    "        self.pnet_detector = pnet\n",
    "        self.rnet_detector = rnet\n",
    "        self.onet_detector = onet\n",
    "        self.min_face_size = min_face_size\n",
    "        self.stride=stride\n",
    "        self.thresh = threshold\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "\n",
    "    def unique_image_format(self,im):\n",
    "        if not isinstance(im,np.ndarray):\n",
    "            if im.mode == 'I':\n",
    "                im = np.array(im, np.int32, copy=False)\n",
    "            elif im.mode == 'I;16':\n",
    "                im = np.array(im, np.int16, copy=False)\n",
    "            else:\n",
    "                im = np.asarray(im)\n",
    "        return im\n",
    "\n",
    "    def square_bbox(self, bbox):\n",
    "        \n",
    "        square_bbox = bbox.copy()\n",
    "\n",
    "        \n",
    "        \n",
    "        h = bbox[:, 3] - bbox[:, 1] + 1# x2 - x1\n",
    "        w = bbox[:, 2] - bbox[:, 0] + 1# y2 - y1\n",
    "        l = np.maximum(h,w)\n",
    "        square_bbox[:, 0] = bbox[:, 0] + w*0.5 - l*0.5\n",
    "        square_bbox[:, 1] = bbox[:, 1] + h*0.5 - l*0.5\n",
    "\n",
    "        # x2 = x1 + l - 1\n",
    "        # y2 = y1 + l - 1\n",
    "        square_bbox[:, 2] = square_bbox[:, 0] + l - 1\n",
    "        square_bbox[:, 3] = square_bbox[:, 1] + l - 1\n",
    "        return square_bbox\n",
    "\n",
    "\n",
    "    def generate_bounding_box(self, map, reg, scale, threshold):\n",
    "        \n",
    "        stride = 2\n",
    "        cellsize = 12 # receptive field\n",
    "\n",
    "        t_index = np.where(map[:,:,0] > threshold)\n",
    "        \n",
    "        if t_index[0].size == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        \n",
    "        dx1, dy1, dx2, dy2 = [reg[0, t_index[0], t_index[1], i] for i in range(4)]\n",
    "        \n",
    "        reg = np.array([dx1, dy1, dx2, dy2])\n",
    "        \n",
    "        score = map[t_index[0], t_index[1], 0]\n",
    "\n",
    "      \n",
    "        boundingbox = np.vstack([np.round((stride * t_index[1]) / scale),            # x1 of prediction box in original image\n",
    "                                 np.round((stride * t_index[0]) / scale),            # y1 of prediction box in original image\n",
    "                                 np.round((stride * t_index[1] + cellsize) / scale), # x2 of prediction box in original image\n",
    "                                 np.round((stride * t_index[0] + cellsize) / scale), # y2 of prediction box in original image\n",
    "                                                                                     # reconstruct the box in original image\n",
    "                                 score,\n",
    "                                 reg,\n",
    "                                \n",
    "                                 ])\n",
    "\n",
    "        return boundingbox.T\n",
    "\n",
    "\n",
    "    def resize_image(self, img, scale):\n",
    "        #resize nd transform\n",
    "        height, width, channels = img.shape\n",
    "        new_height = int(height * scale)     # resized new height\n",
    "        new_width = int(width * scale)       # resized new width\n",
    "        new_dim = (new_width, new_height)\n",
    "        img_resized = cv2.resize(img, new_dim, interpolation=cv2.INTER_LINEAR)      # resized image\n",
    "        return img_resized\n",
    "\n",
    "\n",
    "    def pad(self, bboxes, w, h):\n",
    "        \n",
    "        # width and height\n",
    "        tmpw = (bboxes[:, 2] - bboxes[:, 0] + 1).astype(np.int32)\n",
    "        tmph = (bboxes[:, 3] - bboxes[:, 1] + 1).astype(np.int32)\n",
    "        numbox = bboxes.shape[0]\n",
    "\n",
    "        dx = np.zeros((numbox, ))\n",
    "        dy = np.zeros((numbox, ))\n",
    "        edx, edy  = tmpw.copy()-1, tmph.copy()-1\n",
    "        \n",
    "        x, y, ex, ey = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n",
    "\n",
    "        tmp_index = np.where(ex > w-1)\n",
    "        edx[tmp_index] = tmpw[tmp_index] + w - 2 - ex[tmp_index]\n",
    "        ex[tmp_index] = w - 1\n",
    "\n",
    "        tmp_index = np.where(ey > h-1)\n",
    "        edy[tmp_index] = tmph[tmp_index] + h - 2 - ey[tmp_index]\n",
    "        ey[tmp_index] = h - 1\n",
    "\n",
    "        tmp_index = np.where(x < 0)\n",
    "        dx[tmp_index] = 0 - x[tmp_index]\n",
    "        x[tmp_index] = 0\n",
    "\n",
    "        tmp_index = np.where(y < 0)\n",
    "        dy[tmp_index] = 0 - y[tmp_index]\n",
    "        y[tmp_index] = 0\n",
    "\n",
    "        return_list = [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph]\n",
    "        return_list = [item.astype(np.int32) for item in return_list]\n",
    "\n",
    "        return return_list\n",
    "\n",
    "\n",
    "    def detect_pnet(self, im):\n",
    "       #face box candidate detection\n",
    "\n",
    "        # original wider face data\n",
    "        h, w, c = im.shape\n",
    "\n",
    "        net_size = 12\n",
    "\n",
    "        current_scale = float(net_size) / self.min_face_size    # find initial scale\n",
    "        im_resized = self.resize_image(im, current_scale) \n",
    "        current_height, current_width, _ = im_resized.shape\n",
    "\n",
    "        # fcn\n",
    "        all_boxes = list()\n",
    "        \n",
    "        while min(current_height, current_width) > net_size:\n",
    "            # print(i)\n",
    "            feed_imgs = []\n",
    "            image_tensor =  convert_image_to_tensor(im_resized)\n",
    "            feed_imgs.append(image_tensor)\n",
    "            feed_imgs = torch.stack(feed_imgs)\n",
    "            feed_imgs = Variable(feed_imgs)\n",
    "\n",
    "            if self.pnet_detector.use_cuda:\n",
    "                feed_imgs = feed_imgs.cuda()\n",
    "\n",
    "           \n",
    "            cls_map, reg = self.pnet_detector(feed_imgs)\n",
    "\n",
    "            cls_map_np =  convert_chwTensor_to_hwcNumpy(cls_map.cpu())\n",
    "            reg_np =  convert_chwTensor_to_hwcNumpy(reg.cpu())\n",
    "            \n",
    "            boxes = self.generate_bounding_box(cls_map_np[ 0, :, :], reg_np, current_scale, self.thresh[0])\n",
    "\n",
    "            # generate pyramid images\n",
    "            current_scale *= self.scale_factor \n",
    "            im_resized = self.resize_image(im, current_scale)\n",
    "            current_height, current_width, _ = im_resized.shape\n",
    "\n",
    "            if boxes.size == 0:\n",
    "                continue\n",
    "\n",
    "            # non-maximum suppresion\n",
    "            keep = nms(boxes[:, :5], 0.5, 'Union')\n",
    "            boxes = boxes[keep]\n",
    "            # print(boxes.shape)\n",
    "            all_boxes.append(boxes)\n",
    " \n",
    "\n",
    "        if len(all_boxes) == 0:\n",
    "            return None, None\n",
    "\n",
    "        all_boxes = np.vstack(all_boxes)\n",
    "       \n",
    "        # merge the detection from first stage\n",
    "        keep = nms(all_boxes[:, 0:5], 0.7, 'Union')\n",
    "        all_boxes = all_boxes[keep]\n",
    "        \n",
    "        bw = all_boxes[:, 2] - all_boxes[:, 0] + 1\n",
    "        bh = all_boxes[:, 3] - all_boxes[:, 1] + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        boxes = np.vstack([all_boxes[:,0],\n",
    "                   all_boxes[:,1],\n",
    "                   all_boxes[:,2],\n",
    "                   all_boxes[:,3],\n",
    "                   all_boxes[:,4],\n",
    "                   \n",
    "                  ])\n",
    "\n",
    "        boxes = boxes.T\n",
    "\n",
    "        # boxes = boxes = [x1, y1, x2, y2, score, reg] reg= [px1, py1, px2, py2] (in prediction)\n",
    "        align_topx = all_boxes[:, 0] + all_boxes[:, 5] * bw\n",
    "        align_topy = all_boxes[:, 1] + all_boxes[:, 6] * bh\n",
    "        align_bottomx = all_boxes[:, 2] + all_boxes[:, 7] * bw\n",
    "        align_bottomy = all_boxes[:, 3] + all_boxes[:, 8] * bh\n",
    "\n",
    "        # refine the boxes\n",
    "        boxes_align = np.vstack([ align_topx,\n",
    "                              align_topy,\n",
    "                              align_bottomx,\n",
    "                              align_bottomy,\n",
    "                              all_boxes[:, 4],\n",
    "                             \n",
    "                              ])\n",
    "        boxes_align = boxes_align.T\n",
    "        \n",
    "        #remove invalid box\n",
    "        valindex = [True for _ in range(boxes_align.shape[0])]   \n",
    "        for i in range(boxes_align.shape[0]):\n",
    "            if boxes_align[i][2]-boxes_align[i][0]<=3 or boxes_align[i][3]-boxes_align[i][1]<=3:\n",
    "                valindex[i]=False\n",
    "                print('pnet has one smaller than 3')\n",
    "            else:\n",
    "                if boxes_align[i][2]<1 or boxes_align[i][0]>w-2 or boxes_align[i][3]<1 or boxes_align[i][1]>h-2:\n",
    "                    valindex[i]=False\n",
    "                    print('pnet has one out')\n",
    "        boxes_align=boxes_align[valindex,:]\n",
    "        boxes = boxes[valindex,:]\n",
    "\n",
    "        return boxes, boxes_align\n",
    "\n",
    "    def detect_rnet(self, im, dets):\n",
    "       #seect the face box candidate\n",
    "        # im: an input image\n",
    "        h, w, c = im.shape\n",
    "\n",
    "        if dets is None:\n",
    "            return None,None\n",
    "        if dets.shape[0]==0:\n",
    "            return None, None\n",
    "\n",
    "        # return square boxes\n",
    "        dets = self.square_bbox(dets)\n",
    "        # rounds\n",
    "        dets[:, 0:4] = np.round(dets[:, 0:4])\n",
    "\n",
    "        [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)\n",
    "        num_boxes = dets.shape[0]\n",
    "\n",
    "        # cropped_ims_tensors = np.zeros((num_boxes, 3, 24, 24), dtype=np.float32)\n",
    "        cropped_ims_tensors = []\n",
    "        for i in range(num_boxes):\n",
    "            try:\n",
    "                tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)\n",
    "                tmp[dy[i]:edy[i]+1, dx[i]:edx[i]+1, :] = im[y[i]:ey[i]+1, x[i]:ex[i]+1, :]\n",
    "            except:    \n",
    "                print(dy[i],edy[i],dx[i],edx[i],y[i],ey[i],x[i],ex[i],tmpw[i],tmph[i])\n",
    "                print(dets[i])\n",
    "                print(detss[i])\n",
    "                print(detsss[i])\n",
    "                print(h,w)\n",
    "                exit()\n",
    "            crop_im = cv2.resize(tmp, (24, 24))\n",
    "            crop_im_tensor = convert_image_to_tensor(crop_im)\n",
    "            \n",
    "            cropped_ims_tensors.append(crop_im_tensor)\n",
    "        feed_imgs = Variable(torch.stack(cropped_ims_tensors))\n",
    "\n",
    "        if self.rnet_detector.use_cuda:\n",
    "            feed_imgs = feed_imgs.cuda()\n",
    "\n",
    "        cls_map, reg = self.rnet_detector(feed_imgs)\n",
    "\n",
    "        cls_map = cls_map.cpu().data.numpy()\n",
    "        reg = reg.cpu().data.numpy()\n",
    "\n",
    "        keep_inds = np.where(cls_map > self.thresh[1])[0]\n",
    "\n",
    "        if len(keep_inds) > 0:\n",
    "            boxes = dets[keep_inds]\n",
    "            cls = cls_map[keep_inds]\n",
    "            reg = reg[keep_inds]\n",
    "           \n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        keep = nms(boxes, 0.7)\n",
    "\n",
    "        if len(keep) == 0:\n",
    "            return None, None\n",
    "\n",
    "        keep_cls = cls[keep]\n",
    "        keep_boxes = boxes[keep]\n",
    "        keep_reg = reg[keep]\n",
    "     \n",
    "\n",
    "\n",
    "        bw = keep_boxes[:, 2] - keep_boxes[:, 0] + 1\n",
    "        bh = keep_boxes[:, 3] - keep_boxes[:, 1] + 1\n",
    "\n",
    "\n",
    "        boxes = np.vstack([ keep_boxes[:,0],\n",
    "                              keep_boxes[:,1],\n",
    "                              keep_boxes[:,2],\n",
    "                              keep_boxes[:,3],\n",
    "                              keep_cls[:,0],\n",
    "                             \n",
    "                            ])\n",
    "\n",
    "        align_topx = keep_boxes[:,0] + keep_reg[:,0] * bw\n",
    "        align_topy = keep_boxes[:,1] + keep_reg[:,1] * bh\n",
    "        align_bottomx = keep_boxes[:,2] + keep_reg[:,2] * bw\n",
    "        align_bottomy = keep_boxes[:,3] + keep_reg[:,3] * bh\n",
    "\n",
    "        boxes_align = np.vstack([align_topx,\n",
    "                               align_topy,\n",
    "                               align_bottomx,\n",
    "                               align_bottomy,\n",
    "                               keep_cls[:, 0],\n",
    "                               \n",
    "                             ])\n",
    "\n",
    "        boxes = boxes.T\n",
    "        boxes_align = boxes_align.T\n",
    "        #remove invalid box\n",
    "        valindex = [True for _ in range(boxes_align.shape[0])]   \n",
    "        for i in range(boxes_align.shape[0]):\n",
    "            if boxes_align[i][2]-boxes_align[i][0]<=3 or boxes_align[i][3]-boxes_align[i][1]<=3:\n",
    "                valindex[i]=False\n",
    "                print('rnet has one smaller than 3')\n",
    "            else:\n",
    "                if boxes_align[i][2]<1 or boxes_align[i][0]>w-2 or boxes_align[i][3]<1 or boxes_align[i][1]>h-2:\n",
    "                    valindex[i]=False\n",
    "                    print('rnet has one out')\n",
    "        boxes_align=boxes_align[valindex,:]\n",
    "        boxes = boxes[valindex,:]\n",
    "\n",
    "        return boxes, boxes_align\n",
    "\n",
    "    def detect_onet(self, im, dets):\n",
    "        # get the final face box\n",
    "        h, w, c = im.shape\n",
    "\n",
    "        if dets is None:\n",
    "            return None, None\n",
    "        if dets.shape[0]==0:\n",
    "            return None, None\n",
    "\n",
    "        dets = self.square_bbox(dets)\n",
    "        dets[:, 0:4] = np.round(dets[:, 0:4])\n",
    "\n",
    "        [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)\n",
    "        num_boxes = dets.shape[0]\n",
    "\n",
    "\n",
    "        # cropped_ims_tensors = np.zeros((num_boxes, 3, 24, 24), dtype=np.float32)\n",
    "        cropped_ims_tensors = []\n",
    "        for i in range(num_boxes):\n",
    "            try:\n",
    "                tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)\n",
    "                # crop input image\n",
    "                tmp[dy[i]:edy[i] + 1, dx[i]:edx[i] + 1, :] = im[y[i]:ey[i] + 1, x[i]:ex[i] + 1, :]\n",
    "            except:\n",
    "                print(dy[i],edy[i],dx[i],edx[i],y[i],ey[i],x[i],ex[i],tmpw[i],tmph[i])\n",
    "                print(dets[i])\n",
    "                print(detss[i])\n",
    "                print(h,w)\n",
    "            crop_im = cv2.resize(tmp, (48, 48))\n",
    "            crop_im_tensor = convert_image_to_tensor(crop_im)\n",
    "           \n",
    "            cropped_ims_tensors.append(crop_im_tensor)\n",
    "        feed_imgs = Variable(torch.stack(cropped_ims_tensors))\n",
    "        \n",
    "        if self.rnet_detector.use_cuda:\n",
    "            feed_imgs = feed_imgs.cuda()\n",
    "\n",
    "        cls_map, reg, landmark = self.onet_detector(feed_imgs)\n",
    "\n",
    "        cls_map = cls_map.cpu().data.numpy()\n",
    "        reg = reg.cpu().data.numpy()\n",
    "        landmark = landmark.cpu().data.numpy()\n",
    "        \n",
    "\n",
    "        keep_inds = np.where(cls_map > self.thresh[2])[0]\n",
    "\n",
    "        if len(keep_inds) > 0:\n",
    "            boxes = dets[keep_inds]\n",
    "            cls = cls_map[keep_inds]\n",
    "            reg = reg[keep_inds]\n",
    "            landmark = landmark[keep_inds]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        keep = nms(boxes, 0.7, mode=\"Minimum\")\n",
    "\n",
    "        if len(keep) == 0:\n",
    "            return None, None\n",
    "\n",
    "        keep_cls = cls[keep]\n",
    "        keep_boxes = boxes[keep]\n",
    "        keep_reg = reg[keep]\n",
    "        keep_landmark = landmark[keep]\n",
    "\n",
    "        bw = keep_boxes[:, 2] - keep_boxes[:, 0] + 1\n",
    "        bh = keep_boxes[:, 3] - keep_boxes[:, 1] + 1\n",
    "\n",
    "\n",
    "        align_topx = keep_boxes[:, 0] + keep_reg[:, 0] * bw\n",
    "        align_topy = keep_boxes[:, 1] + keep_reg[:, 1] * bh\n",
    "        align_bottomx = keep_boxes[:, 2] + keep_reg[:, 2] * bw\n",
    "        align_bottomy = keep_boxes[:, 3] + keep_reg[:, 3] * bh\n",
    "\n",
    "        align_landmark_topx = keep_boxes[:, 0]\n",
    "        align_landmark_topy = keep_boxes[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        boxes_align = np.vstack([align_topx,\n",
    "                                 align_topy,\n",
    "                                 align_bottomx,\n",
    "                                 align_bottomy,\n",
    "                                 keep_cls[:, 0],\n",
    "                                 ])\n",
    "\n",
    "        boxes_align = boxes_align.T\n",
    "\n",
    "        landmark =  np.vstack([\n",
    "                                 align_landmark_topx + keep_landmark[:, 0] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 1] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 2] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 3] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 4] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 5] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 6] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 7] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 8] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 9] * bh,\n",
    "                                 ])\n",
    "\n",
    "        landmark_align = landmark.T\n",
    "        \n",
    "\n",
    "        return boxes_align, landmark_align\n",
    "\n",
    "\n",
    "    def detect_face(self,img):\n",
    "        # detect the face\n",
    "        boxes_align = np.array([])\n",
    "        landmark_align =np.array([])\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # pnet\n",
    "        if self.pnet_detector:\n",
    "            boxes, boxes_align = self.detect_pnet(img)\n",
    "            if boxes_align is None:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            t1 = time.time() - t\n",
    "            t = time.time()\n",
    "\n",
    "        # rnet\n",
    "        if self.rnet_detector:\n",
    "            boxes, boxes_align = self.detect_rnet(img, boxes_align)\n",
    "            if boxes_align is None:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            t2 = time.time() - t\n",
    "            t = time.time()\n",
    "\n",
    "        # onet\n",
    "        if self.onet_detector:\n",
    "            boxes_align, landmark_align = self.detect_onet(img, boxes_align)\n",
    "\n",
    "            if boxes_align is None:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            t3 = time.time() - t\n",
    "            t = time.time()\n",
    "            print(\"time cost \" + '{:.3f}'.format(t1+t2+t3) + '  pnet {:.3f}  rnet {:.3f}  onet {:.3f}'.format(t1, t2, t3))\n",
    "\n",
    "        return boxes_align, landmark_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "def convert_image_to_tensor(image):\n",
    "    \"\"\"convert an image to pytorch tensor\n",
    "        Parameters:\n",
    "        ----------\n",
    "        image: numpy array , h * w * c\n",
    "        Returns:\n",
    "        -------\n",
    "        image_tensor: pytorch.FloatTensor, c * h * w\n",
    "        \"\"\"\n",
    "    # image = image.astype(np.float32)\n",
    "    return transform(image)\n",
    "    # return transform(image)\n",
    "\n",
    "\n",
    "def convert_chwTensor_to_hwcNumpy(tensor):\n",
    "    \"\"\"convert a group images pytorch tensor(count * c * h * w) to numpy array images(count * h * w * c)\n",
    "            Parameters:\n",
    "            ----------\n",
    "            tensor: numpy array , count * c * h * w\n",
    "            Returns:\n",
    "            -------\n",
    "            numpy array images: count * h * w * c\n",
    "            \"\"\"\n",
    "\n",
    "    if isinstance(tensor, Variable):\n",
    "        return np.transpose(tensor.data.numpy(), (0,2,3,1))\n",
    "    elif isinstance(tensor, torch.FloatTensor):\n",
    "        return np.transpose(tensor.numpy(), (0,2,3,1))\n",
    "    else:\n",
    "        raise Exception(\"covert b*c*h*w tensor to b*h*w*c numpy error.This tensor must have 4 dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# the basic tools\n",
    "def IoU(box, boxes):\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    \n",
    "    # the offset of the interception of union between crop_box and gt_box\n",
    "    xx1 = np.maximum(box[0], boxes[:, 0])\n",
    "    yy1 = np.maximum(box[1], boxes[:, 1])\n",
    "    xx2 = np.minimum(box[2], boxes[:, 2])\n",
    "    yy2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    # the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    ovr = np.true_divide(inter,(box_area + area - inter))\n",
    "    return ovr\n",
    "\n",
    "\n",
    "def convert_to_square(bbox):\n",
    "    \n",
    "    square_bbox = bbox.copy()\n",
    "\n",
    "    h = bbox[:, 3] - bbox[:, 1] + 1\n",
    "    w = bbox[:, 2] - bbox[:, 0] + 1\n",
    "    max_side = np.maximum(h,w)\n",
    "    square_bbox[:, 0] = bbox[:, 0] + w*0.5 - max_side*0.5\n",
    "    square_bbox[:, 1] = bbox[:, 1] + h*0.5 - max_side*0.5\n",
    "    square_bbox[:, 2] = square_bbox[:, 0] + max_side - 1\n",
    "    square_bbox[:, 3] = square_bbox[:, 1] + max_side - 1\n",
    "    return square_bbox\n",
    "\n",
    "def nms(dets, thresh, mode=\"Union\"):\n",
    "    \n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1] \n",
    "   \n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        \n",
    "        if mode == \"Union\":\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        elif mode == \"Minimum\":\n",
    "            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n",
    "        \n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1] \n",
    "        # print(inds)\n",
    "        \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "p_model_path = \"./model_store/pnet_epoch_10.pt\"\n",
    "r_model_path = \"./model_store/rnet_epoch_10.pt\"\n",
    "o_model_path = \"./model_store/onet_model_final.pt\"#\"./model_store/landmark/onet_epoch_10.pt\"\n",
    "pnet, rnet, onet = create_mtcnn_net(p_model_path=p_model_path, r_model_path=r_model_path, o_model_path=o_model_path, use_cuda=False)\n",
    "mtcnn_detector = MtcnnDetector(pnet=pnet, rnet=rnet, onet=onet, min_face_size=24,threshold=[0.6, 0.7, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_capture = cv2.VideoCapture(0)\n",
    "capture_interval = 1\n",
    "capture_num = 100\n",
    "capture_count = 0\n",
    "frame_count = 0\n",
    "detect_multiple_faces = False #因为是训练目标对象，一次只有一张人脸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 0.844  pnet 0.400  rnet 0.419  onet 0.025\n",
      "time cost 0.878  pnet 0.403  rnet 0.450  onet 0.025\n",
      "time cost 0.859  pnet 0.393  rnet 0.444  onet 0.023\n",
      "time cost 0.870  pnet 0.407  rnet 0.442  onet 0.021\n",
      "time cost 0.832  pnet 0.387  rnet 0.414  onet 0.031\n",
      "time cost 0.852  pnet 0.388  rnet 0.431  onet 0.032\n",
      "time cost 0.774  pnet 0.368  rnet 0.381  onet 0.025\n",
      "time cost 0.782  pnet 0.369  rnet 0.391  onet 0.022\n",
      "time cost 0.762  pnet 0.374  rnet 0.361  onet 0.027\n",
      "time cost 0.741  pnet 0.359  rnet 0.356  onet 0.026\n",
      "time cost 0.770  pnet 0.368  rnet 0.381  onet 0.021\n",
      "time cost 0.809  pnet 0.367  rnet 0.418  onet 0.025\n",
      "time cost 0.866  pnet 0.391  rnet 0.449  onet 0.026\n",
      "time cost 0.867  pnet 0.387  rnet 0.452  onet 0.028\n",
      "time cost 0.926  pnet 0.417  rnet 0.483  onet 0.027\n",
      "time cost 0.929  pnet 0.401  rnet 0.500  onet 0.028\n",
      "time cost 0.860  pnet 0.399  rnet 0.445  onet 0.016\n",
      "time cost 0.873  pnet 0.398  rnet 0.455  onet 0.020\n",
      "time cost 0.882  pnet 0.400  rnet 0.464  onet 0.018\n",
      "time cost 0.873  pnet 0.398  rnet 0.456  onet 0.019\n",
      "time cost 0.938  pnet 0.407  rnet 0.504  onet 0.026\n",
      "time cost 0.924  pnet 0.415  rnet 0.483  onet 0.026\n",
      "time cost 0.967  pnet 0.415  rnet 0.535  onet 0.017\n",
      "time cost 0.931  pnet 0.417  rnet 0.503  onet 0.011\n",
      "time cost 0.919  pnet 0.404  rnet 0.499  onet 0.017\n",
      "time cost 0.958  pnet 0.413  rnet 0.521  onet 0.024\n",
      "time cost 0.935  pnet 0.422  rnet 0.491  onet 0.022\n",
      "time cost 0.961  pnet 0.414  rnet 0.526  onet 0.022\n",
      "time cost 0.926  pnet 0.415  rnet 0.484  onet 0.026\n",
      "time cost 0.922  pnet 0.404  rnet 0.498  onet 0.020\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "capture_interval = 1\n",
    "capture_num = 100\n",
    "capture_count = 0\n",
    "frame_count = 0\n",
    "detect_multiple_faces = False\n",
    "cap = cv2.VideoCapture('./trial.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)#29.97\n",
    "ret, frame = cap.read()\n",
    "if not os.path.exists('./image_store'):\n",
    "    os.makedirs('./image_store')\n",
    "f=open(\"box.txt\",\"w\")\n",
    "f1=open(\"landmark.txt\",\"w\")\n",
    "f4=open('number.txt','w')\n",
    "while True:\n",
    "\n",
    "    #ret, frame = video_capture.read()\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    #每1帧采集一张人脸，这里采样不进行灰度变换，直接保存彩色图\n",
    "    if(capture_count%capture_interval == 0): \n",
    "        \n",
    "        bboxs, landmarks = mtcnn_detector.detect_face(frame)\n",
    "        nrof_faces = bboxs.shape[0]\n",
    "        #print(nrof_faces)\n",
    "        \n",
    "        with open(\"box.txt\",\"a+\") as f:\n",
    "            f.writelines(str(bboxs.tolist())+'\\n')\n",
    "\n",
    "            \n",
    "        with open(\"landmark.txt\",\"a+\") as f1:\n",
    "            f1.writelines(str(landmarks.tolist())+'\\n')\n",
    "            \n",
    "        with open('number.txt','a+') as f4:\n",
    "            f4.writelines(str(nrof_faces)+'\\n')\n",
    "                          \n",
    "        for face_position in bboxs: #因为只采集一张人脸，所以实际只遍历一次\n",
    "            face_position=face_position.astype(int)\n",
    "            cropped = frame[face_position[1]:face_position[3],face_position[0]:face_position[2],:]\n",
    "            scaled = cv2.resize(cropped, (160, 160), interpolation=cv2.INTER_CUBIC )  #这里取和负样本一样大小\n",
    "            name='./image_store/try'+str(capture_count)\n",
    "            if not os.path.exists(name):\n",
    "                os.makedirs(name)\n",
    "            cv2.imwrite(os.path.join(name,str(frame_count)+'.jpg'), scaled)\n",
    "            \n",
    "            frame_count += 1\n",
    "          \n",
    "    capture_count += 1\n",
    "   \n",
    "    if capture_count >= fps:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['number', 'f', 'box']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "import itertools\n",
    "import time\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Function\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet152(nn.Module):\n",
    "    def __init__(self, embedding_dim = 512, pretrained = False):\n",
    "        super(Resnet152, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.resnet152 = models.resnet152(pretrained=pretrained)\n",
    "        self.linear = nn.Linear(self.resnet152.fc.in_features, embedding_dim)\n",
    "        self.resnet152.fc = self.linear\n",
    "        # self.batch_norm = nn.BatchNorm1d(embedding_dim, momentum=0.01)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0.0, 0.02)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        embed = self.resnet152(images)\n",
    "        # embed = self.batch_norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, cnn):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding = cnn\n",
    "\n",
    "    # def forward(self, images_tensor, minibatch_X):\n",
    "    def forward(self, images_tensor):\n",
    "        embeds = self.embedding(images_tensor)\n",
    "        # id2embeds = {}\n",
    "        # minibatch_size = len(minibatch_X)\n",
    "        # for i in range(minibatch_size):\n",
    "        #    x = minibatch_X[i]\n",
    "        #    id2embeds[x] = embeds[i, :]\n",
    "        # return id2embeds\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, alpha = 0.2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        alpha = self.alpha\n",
    "        pos_dist = anchor - positive\n",
    "        pos_dist = torch.pow(pos_dist, 2).sum(dim=1)\n",
    "        neg_dist = anchor - negative\n",
    "        neg_dist = torch.pow(neg_dist, 2).sum(dim=1)\n",
    "        basic_loss = pos_dist - neg_dist + alpha\n",
    "        # loss = torch.clamp(basic_loss, min=0.0).sum()\n",
    "        relu = nn.ReLU()\n",
    "        loss = relu(basic_loss)\n",
    "        return loss.mean()\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, alpha=0.2):\n",
    "    return TripletLoss(alpha)(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data, seed = 0):\n",
    "    image_ids, labels = data\n",
    "    shuffled_image_ids = []\n",
    "    shuffled_labels = []\n",
    "    num_images = len(image_ids)\n",
    "    torch.manual_seed(seed)\n",
    "    perm = list(torch.randperm(num_images))\n",
    "    for i in range(num_images):\n",
    "        shuffled_image_ids.append(image_ids[perm[i]])\n",
    "        shuffled_labels.append(labels[perm[i]])\n",
    "    return shuffled_image_ids, shuffled_labels\n",
    "\n",
    "def make_minibatches(data, minibatch_size = 16,  seed = 0, shuffle = 'random'):\n",
    "    X, Y = data\n",
    "    m = len(X)\n",
    "    minibatches = []\n",
    "    if shuffle == 'sequential':\n",
    "        shuffled_X, shuffled_Y = X, Y\n",
    "\n",
    "    elif shuffle == 'random':\n",
    "        shuffled_X, shuffled_Y = shuffle_data(data, seed = seed)\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/minibatch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[k * minibatch_size : k * minibatch_size + minibatch_size]\n",
    "        minibatch_Y = shuffled_Y[k * minibatch_size : k * minibatch_size + minibatch_size]\n",
    "        minibatches.append((minibatch_X, minibatch_Y))\n",
    "\n",
    "    rem_size = m - num_complete_minibatches * minibatch_size\n",
    "    if m % minibatch_size != 0:\n",
    "        minibatch_X = shuffled_X[num_complete_minibatches * minibatch_size : m]\n",
    "        minibatch_Y = shuffled_Y[num_complete_minibatches * minibatch_size : m]\n",
    "        minibatches.append((minibatch_X, minibatch_Y))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def batch2images_tensor(minibatch_X, dataloader, gpu_device):\n",
    "    minibatch_size = len(minibatch_X)\n",
    "    images_tensor = torch.zeros(minibatch_size, 3, 224, 224)\n",
    "    for i in range(minibatch_size):\n",
    "        x = minibatch_X[i]\n",
    "        x_image = dataloader.get_image(x)\n",
    "        images_tensor[i, :, :, :] = x_image\n",
    "    images_tensor = Variable(images_tensor)\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.cuda.device(gpu_device):\n",
    "            images_tensor = images_tensor.cuda()\n",
    "    return images_tensor\n",
    "\n",
    "def gen_triplets(minibatch, id2embeds, embedding_dim, device, mode = 'all'):\n",
    "    X, Y = minibatch\n",
    "    Y_prod = itertools.product(Y, repeat=3)\n",
    "    X_prod = itertools.product(X, repeat=3)\n",
    "    triplet = []\n",
    "    #print(minibatch)\n",
    "    for x, y  in zip(X_prod, Y_prod):\n",
    "        \n",
    "        xa, xp, xn = x\n",
    "        ya, yp, yn = y\n",
    "        if (ya == yp) and (ya!=yn) and (xa!=xp):\n",
    "            triplet.append((xa, xp, xn))\n",
    "    #print(triplet)\n",
    "    num_triplets = len(triplet)\n",
    "    anchor = torch.zeros(num_triplets, embedding_dim)\n",
    "    positive = torch.zeros(num_triplets, embedding_dim)\n",
    "    negative = torch.zeros(num_triplets, embedding_dim)\n",
    "#     if torch.cuda.is_available():\n",
    "#         with torch.cuda.device(device):\n",
    "#             anchor = anchor.cuda()\n",
    "#             positive = positive.cuda()\n",
    "#             negative = negative.cuda()\n",
    "\n",
    "    for i in range(num_triplets):\n",
    "        xa, xp, xn = triplet[i]\n",
    "        anchor[i, :] = id2embeds[xa]\n",
    "        positive[i, :] = id2embeds[xp]\n",
    "        negative[i, :] = id2embeds[xn]\n",
    "            \n",
    "    return anchor, positive, negative\n",
    "\n",
    "def label2embeds_list2dict(labels_list, embeds):\n",
    "    label2embeds = {}\n",
    "    num_labels = len(labels_list)\n",
    "    for i in range(num_labels):\n",
    "        label = labels_list[i]\n",
    "        label2embeds[label] = embeds[i, :]\n",
    "    return label2embeds\n",
    "\n",
    "def final_label2embeds(triplet_net, train_dataloader, gpu_device):\n",
    "    labels_list = []\n",
    "    image_ids = []\n",
    "    for label, images_list in train_dataloader.images_dict.items():\n",
    "        image_ids.append(images_list[0])\n",
    "        labels_list.append(label)\n",
    "\n",
    "    images_tensor = batch2images_tensor(image_ids, train_dataloader, gpu_device)\n",
    "    with torch.no_grad():\n",
    "        embeds = triplet_net.embedding(images_tensor)\n",
    "    label2embeds = label2embeds_list2dict(labels_list, embeds)\n",
    "    return label2embeds\n",
    "\n",
    "def who_is_it(label2embeds, embed):\n",
    "    labels = []\n",
    "    num_labels = len(label2embeds)\n",
    "    embedding_dim = embed.shape[0]\n",
    "    embeds = torch.zeros(num_labels, embedding_dim)\n",
    "    i = 0\n",
    "    for label, cur_embed in label2embeds.items():\n",
    "        labels.append(label)\n",
    "        embeds[i, :] = cur_embed\n",
    "        i += 1\n",
    "    dist = torch.pow(embeds - embed, 2).sum(dim = 1)\n",
    "    index = torch.argmin(dist).tolist()\n",
    "    return labels[index]\n",
    "\n",
    "def accuracy(data, dataloader, label2embeds, triplet_net, gpu_device):\n",
    "    image_ids, Y = data\n",
    "    num_data = len(Y)\n",
    "    embedding_dim = triplet_net.embedding.embedding_dim\n",
    "    embeds = torch.zeros(num_data, embedding_dim)\n",
    "    minibatch_size = 32\n",
    "    minibatches = make_minibatches(data, minibatch_size = minibatch_size,  seed = 0, shuffle = 'sequential')\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for cur_minibatch in minibatches:\n",
    "        minibatch_X, _ = cur_minibatch\n",
    "        cur_minibatch_size = len(minibatch_X)\n",
    "        end += cur_minibatch_size\n",
    "        images_tensor = batch2images_tensor(minibatch_X, dataloader, gpu_device)\n",
    "        with torch.no_grad():\n",
    "            embeds[start:end, :] = triplet_net(images_tensor)\n",
    "        start = end\n",
    "\n",
    "    acc = 0\n",
    "    pred = []\n",
    "\n",
    "    for i in range(num_data):\n",
    "        embed = embeds[i]\n",
    "        target_label = Y[i]\n",
    "        predicted_label = who_is_it(label2embeds, embed)\n",
    "        if predicted_label == target_label:\n",
    "            acc += 1\n",
    "            pred.append(predicted_label)\n",
    "    return acc, num_data,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dir_path, transform):\n",
    "        self.images_dict = {}\n",
    "        self.id2image = {}\n",
    "        self.labels = None\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        # returns labels/names list\n",
    "        self.labels = os.listdir(self.dir_path)\n",
    "        for label in self.labels:\n",
    "            \n",
    "            path = os.path.join(self.dir_path, label)\n",
    "            if 'DS_Store' in path:\n",
    "                continue\n",
    "            images = os.listdir(path)\n",
    "            self.images_dict[label] = images\n",
    "            for image_id in images:\n",
    "                img_path = os.path.join(path, image_id)\n",
    "                self.id2image[image_id] = self.transform(Image.open(img_path))\n",
    "\n",
    "    def gen_data(self):\n",
    "        labels = []\n",
    "        image_ids = []\n",
    "        for label, images in self.images_dict.items():\n",
    "            num_images = len(images)\n",
    "            labels.extend([label] * num_images)\n",
    "            image_ids.extend(images)\n",
    "        return image_ids, labels\n",
    "\n",
    "    def get_image(self, image_id):\n",
    "        return self.id2image[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2embeds = torch.load('./model_store/iter_1499_label2embeds.pkl',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './image_store'\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                        (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "image_dataloader = DataLoader(image_dir, transform)\n",
    "image_data = image_dataloader.gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(data, dataloader, label2embeds, triplet_net, gpu_device):\n",
    "    \n",
    "    image_ids, Y = data\n",
    "    num_data = len(Y)\n",
    "    embedding_dim = triplet_net.embedding.embedding_dim\n",
    "    embeds = torch.zeros(num_data, embedding_dim)\n",
    "    minibatch_size = 32\n",
    "    minibatches = make_minibatches(data, minibatch_size = minibatch_size,  seed = 0, shuffle = 'sequential')\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for cur_minibatch in minibatches:\n",
    "        minibatch_X, _ = cur_minibatch\n",
    "        cur_minibatch_size = len(minibatch_X)\n",
    "        end += cur_minibatch_size\n",
    "        images_tensor = batch2images_tensor(minibatch_X, dataloader, gpu_device)\n",
    "        with torch.no_grad():\n",
    "            embeds[start:end, :] = triplet_net(images_tensor)\n",
    "        start = end\n",
    "\n",
    "#     acc = 0\n",
    "#     pred = []\n",
    "\n",
    "#     for i in range(num_data):\n",
    "#         embed = embeds[i]\n",
    "#         target_label = Y[i]\n",
    "#         predicted_label = who_is_it(label2embeds, embed)\n",
    "#         if predicted_label == target_label:\n",
    "#             acc += 1\n",
    "#             pred.append(predicted_label)\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn = Resnet152(embedding_dim = embedding_dim, pretrained = False)\n",
    "triplet_net = TripletNet(cnn)\n",
    "embeds=trial(image_data, image_dataloader, label2embeds, triplet_net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 64])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./embedding.txt','w') as f2:\n",
    "    for i in embeds:\n",
    "        f2.writelines(str(i.tolist())+('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strList(s):\n",
    "    '''\n",
    "\n",
    "    :param s:\n",
    "    :return:\n",
    "    '''\n",
    "    s = s.replace('[', '').replace(']', '')\n",
    "    s = s.replace('\\n','')\n",
    "    files = s.split(',') # 用,号分割成list\n",
    "    file=[]\n",
    "    for i in files:\n",
    "        i=float(i)\n",
    "        file.append(i)\n",
    "    files_List = [] # 新建空list\n",
    "    i=0\n",
    "    while i < len(files):\n",
    "        \n",
    "        files_List.append(file[i])\n",
    "        i+=1\n",
    "    return files_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = []\n",
    "for line in open(\"embedding.txt\",\"r\"): #设置文件对象并读取每一行文件\n",
    "    \n",
    "    embed.append(strList(line))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strListDemo(s):\n",
    "    '''\n",
    "\n",
    "    :param s:\n",
    "    :return:\n",
    "    '''\n",
    "    s = s.replace('[', '').replace(']', '')\n",
    "    s = s.replace('\\n','')\n",
    "    files = s.split(',') # 用,号分割成list\n",
    "    file=[]\n",
    "    for i in files:\n",
    "        i=float(i)\n",
    "        file.append(i)\n",
    "    files_List = [] # 新建空list\n",
    "    i=0\n",
    "    while i < len(files):\n",
    "        \n",
    "        files_List.append(file[i:i+5])\n",
    "        i=i+5\n",
    "    return files_List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = []\n",
    "\n",
    "for line in open(\"box.txt\",\"r\"): #设置文件对象并读取每一行文件\n",
    "    #a=line\n",
    "    #print(strListDemo(line))\n",
    "    box.append(strListDemo(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[894.4193812608719,\n",
       "  116.81876841187477,\n",
       "  1149.4248317480087,\n",
       "  473.6086235642433,\n",
       "  0.942638635635376]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strListland(s):\n",
    "    '''\n",
    "\n",
    "    :param s:\n",
    "    :return:\n",
    "    '''\n",
    "    s = s.replace('[', '').replace(']', '')\n",
    "    s = s.replace('\\n','')\n",
    "    files = s.split(',') # 用,号分割成list\n",
    "    file=[]\n",
    "    for i in files:\n",
    "        i=float(i)\n",
    "        file.append(i)\n",
    "    files_List = [] # 新建空list\n",
    "    i=0\n",
    "    while i < len(files):\n",
    "        \n",
    "        files_List.append(file[i:i+10])\n",
    "        i=i+10\n",
    "    return files_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark = []\n",
    "for line in open(\"landmark.txt\",\"r\"): #设置文件对象并读取每一行文件\n",
    "    landmark.append(strListland(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = []\n",
    "for line in open(\"number.txt\",\"r\"): #设置文件对象并读取每一行文件\n",
    "    number.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list=[]\n",
    "n=0\n",
    "for i in range(len(box)):#box,landmark,embedding\n",
    "    nu=number[i]\n",
    "    image_dic={}\n",
    "    image_dic['box']=box[i]\n",
    "    image_dic['landmark']=landmark[i]\n",
    "    image_dic['embedding']=embed[n:n+nu]\n",
    "    image_list.append(image_dic)\n",
    "    #print(n)\n",
    "    n+=nu\n",
    "    #print(n)\n",
    "    #print(i+nu)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename='image.json'\n",
    "with open(filename,'w',encoding='utf-8') as file_obj:\n",
    "    json.dump(image_list,file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
